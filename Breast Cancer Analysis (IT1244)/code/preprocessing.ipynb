{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**\n",
    "\n",
    "In this section, we are looking to:\n",
    "1. choose an appropriate feature scaling method for our dataset\n",
    "2. narrow down the number of predicted mislabelled indices\n",
    "3. employ the use of several machine learning models to help in identifying the mislabelled indices\n",
    "4. swap the mislabelled data back to their original value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the top rows of the data to ensure that the diagnosis values had been changed from \"B\" to 0 and \"M\" to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0          1        17.99         10.38          122.80     1001.0   \n",
       "1          1        20.57         17.77          132.90     1326.0   \n",
       "2          1        19.69         21.25          130.00     1203.0   \n",
       "3          1        11.42         20.38           77.58      386.1   \n",
       "4          1        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0         0.2419  ...         25.38          17.33           184.60   \n",
       "1         0.1812  ...         24.99          23.41           158.80   \n",
       "2         0.2069  ...         23.57          25.53           152.50   \n",
       "3         0.2597  ...         14.91          26.50            98.87   \n",
       "4         0.1809  ...         22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"../data/dataset_exploratory.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset into X and y where X contains the data on all 30 features and y contains the data on diagnosis. Additionally, we created a header to store the names of all features for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.loc[:, dataset.columns != \"diagnosis\"]\n",
    "y = dataset.loc[:, dataset.columns == \"diagnosis\"].to_numpy()\n",
    "header = list(dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will scale the features by standardising them since the data has varying range, and that our dataset has outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, in our exploratory data analysis, we were unable to identify any distinct clusters between the features we selected. Now, we will utilise K-means clustering to see if it could split the benign and malignant points into two clusters, and then identifying the mislabelled points from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the K-means algorithm depends mostly on the initialisation of the two centroids, we decided to try 50 different initialisations by running the algorithm 50 times, using different values for random_state in each iteration.\n",
    "\n",
    "Then, within each iteration, it obtains the indices of the rows that produced a false positive or false negative prediction, i.e. when actual = 0, predicted = 1 and actual = 1, predicted = 0 respectively. \n",
    "\n",
    "The indices are saved into two separate dictionaries, frequent_mislabel_B which contains indices of actual = 0, prediction = 1, and frequent_mislabel_M which contains indices of actual = 1, prediction = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxIter = 50\n",
    "\n",
    "false_positives = {}\n",
    "false_negatives = {}\n",
    "\n",
    "for i in range(maxIter):\n",
    "    kmeans = KMeans(n_clusters = 2, random_state = i*10)\n",
    "    kmeans.fit(X_scaled)\n",
    "\n",
    "    dataset[\"cluster\"] = kmeans.labels_\n",
    "\n",
    "    cross_tab = pd.crosstab(dataset[\"diagnosis\"], dataset[\"cluster\"])\n",
    "    \n",
    "    if (cross_tab[0][0] + cross_tab[1][1]) < (cross_tab[0][1] + cross_tab[1][0]):\n",
    "        dataset[\"cluster\"] = dataset[\"cluster\"].map({0: 1, 1: 0})\n",
    "\n",
    "    mislabel_B = dataset[(dataset[\"diagnosis\"] == 0) & (dataset[\"cluster\"] == 1)]\n",
    "    mislabel_M = dataset[(dataset[\"diagnosis\"] == 1) & (dataset[\"cluster\"] == 0)]\n",
    "\n",
    "    indices_B = mislabel_B.index\n",
    "    indices_M = mislabel_M.index\n",
    "\n",
    "    for j in range(len(indices_B)):\n",
    "        if indices_B[j] in false_positives:\n",
    "            false_positives[indices_B[j]] += 1\n",
    "        else:\n",
    "            false_positives[indices_B[j]] = 1\n",
    "\n",
    "    for j in range(len(indices_M)):\n",
    "        if indices_M[j] in false_negatives:\n",
    "            false_negatives[indices_M[j]] += 1\n",
    "        else:\n",
    "            false_negatives[indices_M[j]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reasoning for storing the occurrences in a dictionary is to see if there are any indices that appear less frequently than others. So, we sort the dictionary based on the values in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_false_positives = {k: v for k, v in sorted(false_positives.items(),\n",
    "                                  key=lambda item: item[1])}\n",
    "freq_false_negatives = {k: v for k, v in sorted(false_negatives.items(), \n",
    "                                  key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{176: 22, 247: 22, 505: 22, 541: 22, 47: 50, 53: 50, 65: 50, 68: 50, 81: 50, 89: 50, 112: 50, 128: 50, 152: 50, 161: 50, 214: 50, 242: 50, 290: 50, 318: 50, 368: 50, 376: 50, 421: 50, 465: 50, 485: 50, 504: 50}\n",
      "{43: 28, 13: 50, 16: 50, 38: 50, 39: 50, 40: 50, 41: 50, 44: 50, 54: 50, 63: 50, 73: 50, 74: 50, 86: 50, 99: 50, 100: 50, 119: 50, 124: 50, 126: 50, 135: 50, 143: 50, 171: 50, 182: 50, 184: 50, 186: 50, 195: 50, 205: 50, 207: 50, 234: 50, 255: 50, 261: 50, 263: 50, 268: 50, 274: 50, 277: 50, 284: 50, 293: 50, 333: 50, 350: 50, 385: 50, 414: 50, 431: 50, 435: 50, 444: 50, 450: 50, 456: 50, 470: 50, 483: 50, 489: 50, 497: 50, 511: 50, 536: 50}\n"
     ]
    }
   ],
   "source": [
    "print(freq_false_positives)\n",
    "print(freq_false_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that there are some indices that do not appear very often, and majority of the indices appear in all 50 iterations. We will keep the indices that appear in all 50 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_infreq(dct, maxIter):\n",
    "    output = {}\n",
    "    for key in dct:\n",
    "        if dct[key] == maxIter:\n",
    "            output[key] = maxIter\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_B = remove_infreq(freq_false_positives, maxIter)\n",
    "freq_M = remove_infreq(freq_false_negatives, maxIter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then obtain all the indices that are predicted to be mislabelled by combining the keys of both dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_predicted_indices = []\n",
    "\n",
    "for index in freq_B:\n",
    "    kmeans_predicted_indices.append(index)\n",
    "for index in freq_M:\n",
    "    kmeans_predicted_indices.append(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now validate the indices by comparing it to the actual swapped indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10,  47,  53,  63,  65,  74,  91, 124, 143, 161, 195, 214, 234,\n",
       "       268, 284, 293, 297, 333, 350, 368, 431, 450, 456, 470, 483, 497,\n",
       "       511, 514])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_indices = np.load(\"../data/changed_label_row_inds.npy\")\n",
    "actual_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two functions below are extracted from the exploratory jupyter notebook as we are performing the same type of comparison as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_indices(predicted_indices, actual_indices):\n",
    "    matching_indices = []\n",
    "\n",
    "    for ind in predicted_indices:\n",
    "        if ind in actual_indices:\n",
    "            matching_indices.append(ind)\n",
    "\n",
    "    return sorted(matching_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_indices(matching_indices, actual_indices):\n",
    "    missing_indices = []\n",
    "\n",
    "    for ind in actual_indices:\n",
    "        if ind in actual_indices and ind not in matching_indices:\n",
    "            missing_indices.append(ind)\n",
    "\n",
    "    return sorted(missing_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then obtain the matching indices obtained from the K-means method and the indices that are not present in the matching indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_matching_indices = get_matching_indices(kmeans_predicted_indices, actual_indices)\n",
    "kmeans_missing_indices = get_missing_indices(kmeans_matching_indices, actual_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen below, the ratio of matching to predicted indices is roughly the same as previously obtained in the exploratory data analysis, where the ratio was 26:76 from the feature engineered radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted indices: 70\n",
      "[ 47  53  65  68  81  89 112 128 152 161 214 242 290 318 368 376 421 465\n",
      " 485 504  13  16  38  39  40  41  44  54  63  73  74  86  99 100 119 124\n",
      " 126 135 143 171 182 184 186 195 205 207 234 255 261 263 268 274 277 284\n",
      " 293 333 350 385 414 431 435 444 450 456 470 483 489 497 511 536]\n",
      "\n",
      "matching indices: 24\n",
      "[ 47  53  63  65  74 124 143 161 195 214 234 268 284 293 333 350 368 431\n",
      " 450 456 470 483 497 511]\n",
      "\n",
      "missing indices: 4\n",
      "[ 10  91 297 514]\n"
     ]
    }
   ],
   "source": [
    "print(f'predicted indices: {len(kmeans_predicted_indices)}\\n{np.array(kmeans_predicted_indices)}\\n')\n",
    "print(f'matching indices: {len(kmeans_matching_indices)}\\n{np.array(kmeans_matching_indices)}\\n')\n",
    "print(f'missing indices: {len(kmeans_missing_indices)}\\n{np.array(kmeans_missing_indices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now load the indices predicted from the feature engineered radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10,  12,  13,  14,  36,  38,  39,  41,  47,  53,  63,  65,  74,\n",
       "        81,  83,  86,  92,  99, 122, 124, 135, 143, 146, 161, 177, 190,\n",
       "       194, 195, 197, 200, 202, 204, 212, 213, 214, 215, 225, 234, 268,\n",
       "       277, 284, 293, 329, 333, 340, 347, 350, 351, 359, 368, 372, 385,\n",
       "       389, 414, 430, 431, 432, 450, 451, 456, 465, 470, 472, 476, 479,\n",
       "       481, 483, 497, 509, 511, 513, 514, 518, 532, 536, 549])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feg_predicted_indices = np.load(\"../data/feg_predicted_indices.npy\")\n",
    "feg_predicted_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be some overlap between kmeans_predicted_indices and feg_predicted_indices. Thus, we decided to intersect the two indices by converting them to a set, getting the intersection, and converting it back to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect_predicted_indices = sorted(list(set(feg_predicted_indices).intersection(set(kmeans_predicted_indices))))\n",
    "intersect_matching_indices = get_matching_indices(intersect_predicted_indices, actual_indices)\n",
    "intersect_missing_indices = get_missing_indices(intersect_matching_indices, actual_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen below, the number of matching indices did not change, but we narrowed down the number of predicted indices. This suggests we are perhaps on the right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted indices: 37\n",
      "[ 13  38  39  41  47  53  63  65  74  81  86  99 124 135 143 161 195 214\n",
      " 234 268 277 284 293 333 350 368 385 414 431 450 456 465 470 483 497 511\n",
      " 536]\n",
      "\n",
      "matching indices: 24\n",
      "[ 47  53  63  65  74 124 143 161 195 214 234 268 284 293 333 350 368 431\n",
      " 450 456 470 483 497 511]\n",
      "\n",
      "missing indices: 4\n",
      "[ 10  91 297 514]\n"
     ]
    }
   ],
   "source": [
    "print(f'predicted indices: {len(intersect_predicted_indices)}\\n{np.array(intersect_predicted_indices)}\\n')\n",
    "print(f'matching indices: {len(intersect_matching_indices)}\\n{np.array(intersect_matching_indices)}\\n')\n",
    "print(f'missing indices: {len(intersect_missing_indices)}\\n{np.array(intersect_missing_indices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we decided to employ the logistic regression model onto our dataset, since it is a binary classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the testing data to be the rows of data that have the same indices as those in predicted indices, and every other row of data to be the training data. This makes it such that we train the data based on the non-mislabelled data, and test it on the mislabelled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indices = np.arange(X.shape[0])\n",
    "train_indices = np.setdiff1d(all_indices, intersect_predicted_indices)\n",
    "\n",
    "X_train = X_scaled[train_indices]\n",
    "y_train = y[train_indices]\n",
    "X_test = X_scaled[intersect_predicted_indices]\n",
    "y_test = y[intersect_predicted_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then fitted the training data to the logistic regression model, obtained the predictions given the testing data, and obtained our confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter = 1000, random_state = 23)\n",
    "logreg.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "conf_matrix = confusion_matrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our confusion matrix seems to have a huge proportion of false positive and false negative data, which was what we were hoping to obtain, as there should be a substantial amount of mislabelled data in the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[ 2 25]\n",
      " [ 6  4]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then extracted the indices of the rows that produced the false positives and false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_predicted_indices = []\n",
    "\n",
    "for i in range(y_pred.shape[0]):\n",
    "    if y_pred[i] != y_test[i]:\n",
    "        logreg_predicted_indices.append(intersect_predicted_indices[i])\n",
    "\n",
    "logreg_matching_indices = get_matching_indices(logreg_predicted_indices, actual_indices)\n",
    "logreg_missing_indices = get_missing_indices(logreg_matching_indices, actual_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen below, we have succcesfully narrowed down the number of predicted indices while keeping the matching indices the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted indices: 31\n",
      "[ 13  38  41  47  53  63  65  74  86  99 124 135 143 161 195 214 234 268\n",
      " 284 293 333 350 368 385 431 450 456 470 483 497 511]\n",
      "\n",
      "matching indices: 24\n",
      "[ 47  53  63  65  74 124 143 161 195 214 234 268 284 293 333 350 368 431\n",
      " 450 456 470 483 497 511]\n",
      "\n",
      "missing indices: 4\n",
      "[ 10  91 297 514]\n"
     ]
    }
   ],
   "source": [
    "print(f'predicted indices: {len(logreg_predicted_indices)}\\n{np.array(logreg_predicted_indices)}\\n')\n",
    "print(f'matching indices: {len(logreg_matching_indices)}\\n{np.array(logreg_matching_indices)}\\n')\n",
    "print(f'missing indices: {len(logreg_missing_indices)}\\n{np.array(logreg_missing_indices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then decided to repeat this same process once more to see if it could narrow down the number of predicted indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indices = np.arange(X.shape[0])\n",
    "train_indices = np.setdiff1d(all_indices, logreg_predicted_indices)\n",
    "\n",
    "X_train = X_scaled[train_indices]\n",
    "y_train = y[train_indices]\n",
    "X_test = X_scaled[logreg_predicted_indices]\n",
    "y_test = y[logreg_predicted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter = 1000, random_state = 23)\n",
    "logreg.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "conf_matrix = confusion_matrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[ 0 22]\n",
      " [ 6  3]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg2_predicted_indices = []\n",
    "\n",
    "for i in range(y_pred.shape[0]):\n",
    "    if y_pred[i] != y_test[i]:\n",
    "        logreg2_predicted_indices.append(logreg_predicted_indices[i])\n",
    "\n",
    "logreg2_matching_indices = get_matching_indices(logreg_predicted_indices, actual_indices)\n",
    "logreg2_missing_indices = get_missing_indices(logreg2_matching_indices, actual_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen below, it seems we have succeeded in narrowing the number of predicted indices down. Given that there were 28 mislabelled data, we managed to predict 28 mislabelled data, albeit only 24 matching the actual mislabelled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted indices: 28\n",
      "[ 38  47  53  63  65  74  99 124 135 143 161 195 214 234 268 284 293 333\n",
      " 350 368 385 431 450 456 470 483 497 511]\n",
      "\n",
      "matching indices: 24\n",
      "[ 47  53  63  65  74 124 143 161 195 214 234 268 284 293 333 350 368 431\n",
      " 450 456 470 483 497 511]\n",
      "\n",
      "missing indices: 4\n",
      "[ 10  91 297 514]\n"
     ]
    }
   ],
   "source": [
    "print(f'predicted indices: {len(logreg2_predicted_indices)}\\n{np.array(logreg2_predicted_indices)}\\n')\n",
    "print(f'matching indices: {len(logreg2_matching_indices)}\\n{np.array(logreg2_matching_indices)}\\n')\n",
    "print(f'missing indices: {len(logreg2_missing_indices)}\\n{np.array(logreg2_missing_indices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now swap the mislabelled data back to their original value, so that we can perform our classification task. This is using the indices from the actual mislabelled data, so that we do not harm the quality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in actual_indices:\n",
    "    y[index] = 1 - y[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we split the dataset into X and y earlier, we merged it back into one dataframe to be saved in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_modified_X = pd.DataFrame(X_scaled, columns=header[1:])\n",
    "dataset_modified_Y = pd.DataFrame(y, columns = [\"diagnosis\"])\n",
    "dataset_modified = pd.concat([dataset_modified_Y, dataset_modified_X], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_modified.to_csv(\"../data/dataset_preprocessed.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we applied feature scaling to the dataset, we saved the scaler such that for future data, it can be scaled down in the same way, and then perform predictions based on the new scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"scaler.bin\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
